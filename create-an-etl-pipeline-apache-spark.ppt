# Creating an ETL Pipeline with Amazon EMR and Apache Spark (using PySpark)

In this tutorial, you learned about how you can build an ETL (Extract, Transform, and Load) pipeline for batch processing using Amazon EMR and Spark. During this process we will also learn about few of the use case of batch ETL process and how EMR can be leveraged to solve such problems. 

We are going to use [PySpark](https://spark.apache.org/docs/latest/api/python/) to intract with the Spark cluster. PySpark allows you to write Spark applications using Python APIs. 

## Requirements 

We assume that you have the following:

- An AWS account 
- An IAM user that has the access to create IAM role, which will be used to trigger or execute the Spark job 
- Basic understanding of Python


## Use case and problem statement

For this use case, we will take [YouTube Video Statistics](https://www.kaggle.com/datasets/datasnaek/youtube-new) dataset and shall clean, analyse and process that data to get some insights about different videos we have in that data set. We should be able to query:

1. Which catagoty of the videos are most popular (may be videos with max. no. of `likes`) ?
2. Which catagoty of the videos are not all that popular (may be videos with max. no. of `dislikes`) ?
3. Which are the most-viewed videos overall in a particular year ?

## Explore the dataset 

This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.

Each regionâ€™s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.

The data also includes a `category_id` field, which varies between regions. 

![Img 1](img/1.png)

To retrieve the categories for a specific video, find it in the associated JSON (`id` key for each item).

![Img 2](img/2.png)

Now this dataset is available to us in the `raw` formate in `CSV` and `JSON` respectively, and to make some useful insights from this data, we need to clean and process that data so that we can run some insighful query. And this is where we would use Amazon EMR and PySpark to clean and process the data.

## Architeture 


